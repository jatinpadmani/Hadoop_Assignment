
Hadoop is a framework that allows for the distributed processing of large data sets across clusters of computers.
It is designed to scale up from a single server to thousands of machines, each offering local computation and storage.
The Hadoop framework itself is written in Java, and it provides a programming model called MapReduce for processing large data sets.
MapReduce is a key component of the Hadoop ecosystem.
